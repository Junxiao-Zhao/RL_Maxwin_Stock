model_name: 'Qwen/Qwen2.5-0.5B'
device: 'cuda'
seed: 42

load_pretrained: False
batch_size: 256
learning_rate: 1e-7
dataset_folder: './data'

trial_name: "test"

PatchTSTConfig:
  num_input_channels: 2
  context_length: 32
  patch_length: 8
  patch_stride: 8
  head_dropout: 0.2
  num_targets: 2

dataset:
  train_fp: "${model.dataset_folder}/train.parquet"
  eval_fp: "${model.dataset_folder}/eval.parquet"
  
GRPOConfig:
  output_dir: './checkpoints'
  learning_rate: ${model.learning_rate}
  per_device_train_batch_size: ${model.batch_size}
  per_device_eval_batch_size: ${model.batch_size}
  num_generations: 64
  temperature: 1
  beta: 0.0
  use_vllm: False
  scale_rewards: False
  gradient_accumulation_steps: 8
  num_train_epochs: 3
  logging_strategy: "steps"
  logging_steps: 1
  eval_strategy: "steps"
  eval_steps: 10
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 1
  metric_for_best_model: "eval_reward"
  greater_is_better: True
  load_best_model_at_end: True
  weight_decay: 0.01
  optim: "adamw_torch"
  report_to: "tensorboard"

EarlyStoppingCallback:
  early_stopping_patience: 100

train:
  resume_from_checkpoint: False