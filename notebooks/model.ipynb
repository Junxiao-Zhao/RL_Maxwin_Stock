{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    Qwen2Model,\n",
    "    Qwen2PreTrainedModel,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutput\n",
    "# from transformers.models.qwen2.modeling_qwen2 import Qwen2DecoderLayer\n",
    "from trl import GRPOTrainer\n",
    "from trl.trainer.utils import selective_log_softmax\n",
    "\n",
    "from src.price_process import PriceProcessor\n",
    "from src.model import Qwen2ForAction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program\\miniconda3\\envs\\DL\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"Qwen/Qwen2.5-0.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "    # \"hidden_size\": 2,\n",
    "    \"vocab_size\": 2,\n",
    "    \"num_generations\": 16,\n",
    "    \"prompt_length\": 5,\n",
    "    \"max_length\": 2048,\n",
    "    # \"num_attention_heads\": 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForAction(\n",
       "  (embed): Linear(in_features=2, out_features=896, bias=True)\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(2, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Qwen2ForAction(config).to(\"cuda\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_df = pd.read_csv(\"../data/000300_pe.csv\", encoding=\"gbk\")\n",
    "index_df = pd.read_csv(\"../data/000300_price.csv\", encoding=\"gbk\")\n",
    "bond_df = pd.read_csv(\"../data/10yr_bond_yield.csv\", encoding=\"gbk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_df[\"tradeDate\"] = pd.to_datetime(pe_df[\"tradeDate\"])\n",
    "index_df[\"tradeDate\"] = pd.to_datetime(index_df[\"tradeDate\"])\n",
    "bond_df[\"tradeDate\"] = pd.to_datetime(bond_df[\"tradeDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df = index_df.merge(pe_df, how=\"left\", on=\"tradeDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2974"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df = index_df[index_df[\"tradeDate\"] >= \"2013-01-01\"]\n",
    "len(index_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3031"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bond_df = bond_df[bond_df[\"tradeDate\"] >= \"2013-01-01\"]\n",
    "len(bond_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = index_df.merge(bond_df, how=\"left\", on=\"tradeDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PriceProcessor(overall_df, [\"EPValue\", \"yield\"],\n",
    "                    extra_cols=[\"openIndex\", \"closeIndex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tradeDate': [1738713600.0,\n",
       "  1738800000.0,\n",
       "  1738886400.0,\n",
       "  1739145600.0,\n",
       "  1739232000.0,\n",
       "  1739318400.0,\n",
       "  1739404800.0,\n",
       "  1739491200.0,\n",
       "  1739750400.0,\n",
       "  1739836800.0],\n",
       " 'EPValue': [0.080972,\n",
       "  0.080192,\n",
       "  0.079428,\n",
       "  0.079239,\n",
       "  0.079239,\n",
       "  0.078616,\n",
       "  0.078802,\n",
       "  0.078247,\n",
       "  0.078125,\n",
       "  0.07776],\n",
       " 'yield': [1.63,\n",
       "  1.614,\n",
       "  1.606,\n",
       "  1.623,\n",
       "  1.632,\n",
       "  1.632,\n",
       "  1.634,\n",
       "  1.657,\n",
       "  1.677,\n",
       "  1.702],\n",
       " 'openIndex': [3844.6998,\n",
       "  3789.6086,\n",
       "  3844.0574,\n",
       "  3898.1023,\n",
       "  3905.1594,\n",
       "  3875.9596,\n",
       "  3913.9044,\n",
       "  3900.8274,\n",
       "  3954.4147,\n",
       "  3942.5761],\n",
       " 'closeIndex': [3795.0848,\n",
       "  3842.8314,\n",
       "  3892.7028,\n",
       "  3901.0618,\n",
       "  3883.1352,\n",
       "  3919.8617,\n",
       "  3905.1398,\n",
       "  3939.0085,\n",
       "  3947.3983,\n",
       "  3912.7829],\n",
       " 'input_ids': tensor([[0.0810, 1.6300],\n",
       "         [0.0802, 1.6140],\n",
       "         [0.0794, 1.6060],\n",
       "         [0.0792, 1.6230],\n",
       "         [0.0792, 1.6320],\n",
       "         [0.0786, 1.6320],\n",
       "         [0.0788, 1.6340],\n",
       "         [0.0782, 1.6570],\n",
       "         [0.0781, 1.6770],\n",
       "         [0.0778, 1.7020]]),\n",
       " 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'action_mask': tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0.])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pp(start_time=\"2025-02-01\", prompt_len=5, max_len=10, pad=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = data[\"input_ids\"].unsqueeze(0).to(\"cuda\")\n",
    "attention_mask = data[\"attention_mask\"].unsqueeze(0).to(\"cuda\")\n",
    "output = model(input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_to_keep = 5\n",
    "logits = output.logits[:, :-1, :]\n",
    "input_ids = input_ids[:, -logits_to_keep:]\n",
    "logits = logits[:, -logits_to_keep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0786, 1.6320],\n",
       "         [0.0788, 1.6340],\n",
       "         [0.0782, 1.6570],\n",
       "         [0.0781, 1.6770],\n",
       "         [0.0778, 1.7020]]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 0, 1, 0, 0, 1, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 1, 0, 0, 1]],\n",
       "\n",
       "        [[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 1, 0, 1, 0, 1]],\n",
       "\n",
       "        [[0, 0, 0, 1, 1, 0, 1, 0, 0, 1]]], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = Categorical(output.logits)\n",
    "actions = dist.sample(torch.Size([5]))\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2236, -1.2240, -0.3481, -1.2241, -0.3482, -0.3482, -1.2239,\n",
       "          -0.3484, -0.3486, -0.3488]],\n",
       "\n",
       "        [[-0.3484, -0.3482, -0.3481, -0.3482, -0.3482, -0.3482, -1.2239,\n",
       "          -0.3484, -0.3486, -1.2227]],\n",
       "\n",
       "        [[-1.2236, -1.2240, -0.3481, -0.3482, -0.3482, -0.3482, -0.3483,\n",
       "          -1.2235, -0.3486, -0.3488]],\n",
       "\n",
       "        [[-0.3484, -0.3482, -0.3481, -0.3482, -0.3482, -1.2239, -0.3483,\n",
       "          -1.2235, -0.3486, -1.2227]],\n",
       "\n",
       "        [[-0.3484, -0.3482, -0.3481, -1.2241, -1.2239, -0.3482, -1.2239,\n",
       "          -0.3484, -0.3486, -1.2227]]], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = dist.log_prob(actions)\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = actions.permute(1, 0, 2)\n",
    "log_probs = log_probs.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "         [1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "         [0, 0, 0, 1, 1, 0, 1, 0, 0, 1]]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_mask = data[\"action_mask\"].unsqueeze(0).to(\"cuda\")\n",
    "action_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1, -1, -1, -1,  0,  0,  1,  0,  0, -1],\n",
       "        [-1, -1, -1, -1,  0,  0,  1,  0,  0, -1],\n",
       "        [-1, -1, -1, -1,  0,  0,  0,  1,  0, -1],\n",
       "        [-1, -1, -1, -1,  0,  1,  0,  1,  0, -1],\n",
       "        [-1, -1, -1, -1,  1,  0,  1,  0,  0, -1]], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 1, 0],\n",
       "        [0, 1, 0, 1, 0],\n",
       "        [1, 0, 1, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0].masked_select(action_mask[0] == 1).view(actions.size(1), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1, -1, -1, -1,  0,  0,  1,  0,  0, -1],\n",
       "         [-1, -1, -1, -1,  0,  0,  1,  0,  0, -1],\n",
       "         [-1, -1, -1, -1,  0,  0,  0,  1,  0, -1],\n",
       "         [-1, -1, -1, -1,  0,  1,  0,  1,  0, -1],\n",
       "         [-1, -1, -1, -1,  1,  0,  1,  0,  0, -1]]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.masked_fill_(action_mask.unsqueeze(1) == 0, -1)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_actions = actions[:, :, config.prompt_length - 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0],\n",
       "         [0, 1, 0, 1, 0],\n",
       "         [1, 0, 1, 0, 0]]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  0,  0, -1,  1, -1,  0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([0, 1, 1, 1, 0, 1, 0])\n",
    "is_one = a.eq(1).int()\n",
    "diff = torch.diff(is_one.int(),\n",
    "                  prepend=torch.tensor([0]),\n",
    "                  append=torch.tensor([0]))\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(diff == -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_yield(\n",
    "        actions: torch.Tensor,  # (N, S)\n",
    "        open_price: torch.Tensor,  # (S)\n",
    "        close_price: torch.Tensor,  # (S)\n",
    "        slippage: float = 0.01,\n",
    "        stamps: float = 0.0,\n",
    "        service_fee: float = 1.3e-4,\n",
    "        assets: float = 2e5):\n",
    "\n",
    "    num_generations, span_len = actions.shape\n",
    "\n",
    "    diff = torch.diff(actions.eq(1).int(),\n",
    "                      prepend=torch.zeros((num_generations, 1),\n",
    "                                          device=actions.device),\n",
    "                      append=torch.zeros((num_generations, 1),\n",
    "                                         device=actions.device))\n",
    "\n",
    "    yields = torch.zeros(num_generations).to(actions.device)\n",
    "\n",
    "    for i in range(num_generations):\n",
    "\n",
    "        total_assets = assets\n",
    "\n",
    "        # calculate the holding periods\n",
    "        start = torch.where(diff[i] == 1)[0]\n",
    "        end = torch.where(diff[i] == -1)[0]\n",
    "\n",
    "        for span in list(zip(start.tolist(), end.tolist())):\n",
    "            start_idx, end_idx = span\n",
    "\n",
    "            bid_rate = (open_price[start_idx] + slippage) * (1 + service_fee)\n",
    "            shares = total_assets // bid_rate\n",
    "            total_assets = total_assets % bid_rate\n",
    "\n",
    "            if end_idx < span_len:\n",
    "                ask_rate = (open_price[end_idx] -\n",
    "                            slippage) * (1 - service_fee - stamps)\n",
    "            else:\n",
    "                ask_rate = close_price[-1]\n",
    "\n",
    "            total_assets += shares * ask_rate\n",
    "\n",
    "        yields[i] = (total_assets - assets) / assets\n",
    "\n",
    "    return yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0134,  0.0134, -0.0032, -0.0068,  0.0228], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_yield(\n",
    "    valid_actions[0],\n",
    "    data[\"openIndex\"][-5:],\n",
    "    data[\"closeIndex\"][-5:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 1, 0, 0],\n",
       "         [0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0],\n",
       "         [0, 1, 0, 1, 0],\n",
       "         [1, 0, 1, 0, 0]]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3875.9596, 3913.9044, 3900.8274, 3954.4147, 3942.5761]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"openIndex\"][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tradeDate': [1738713600.0,\n",
       "  1738800000.0,\n",
       "  1738886400.0,\n",
       "  1739145600.0,\n",
       "  1739232000.0,\n",
       "  1739318400.0,\n",
       "  1739404800.0,\n",
       "  1739491200.0,\n",
       "  1739750400.0,\n",
       "  1739836800.0],\n",
       " 'EPValue': [0.080972,\n",
       "  0.080192,\n",
       "  0.079428,\n",
       "  0.079239,\n",
       "  0.079239,\n",
       "  0.078616,\n",
       "  0.078802,\n",
       "  0.078247,\n",
       "  0.078125,\n",
       "  0.07776],\n",
       " 'yield': [1.63,\n",
       "  1.614,\n",
       "  1.606,\n",
       "  1.623,\n",
       "  1.632,\n",
       "  1.632,\n",
       "  1.634,\n",
       "  1.657,\n",
       "  1.677,\n",
       "  1.702],\n",
       " 'openIndex': [3844.6998,\n",
       "  3789.6086,\n",
       "  3844.0574,\n",
       "  3898.1023,\n",
       "  3905.1594,\n",
       "  3875.9596,\n",
       "  3913.9044,\n",
       "  3900.8274,\n",
       "  3954.4147,\n",
       "  3942.5761],\n",
       " 'closeIndex': [3795.0848,\n",
       "  3842.8314,\n",
       "  3892.7028,\n",
       "  3901.0618,\n",
       "  3883.1352,\n",
       "  3919.8617,\n",
       "  3905.1398,\n",
       "  3939.0085,\n",
       "  3947.3983,\n",
       "  3912.7829],\n",
       " 'input_ids': tensor([[0.0810, 1.6300],\n",
       "         [0.0802, 1.6140],\n",
       "         [0.0794, 1.6060],\n",
       "         [0.0792, 1.6230],\n",
       "         [0.0792, 1.6320],\n",
       "         [0.0786, 1.6320],\n",
       "         [0.0788, 1.6340],\n",
       "         [0.0782, 1.6570],\n",
       "         [0.0781, 1.6770],\n",
       "         [0.0778, 1.7020]]),\n",
       " 'attention_mask': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'action_mask': tensor([0., 0., 0., 0., 1., 1., 1., 1., 1., 0.])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union\n",
    "\n",
    "from accelerate.utils.operations import gather\n",
    "from trl import GRPOTrainer\n",
    "from trl.extras.profiling import profiling_context\n",
    "\n",
    "\n",
    "class ActionGRPOTrainer(GRPOTrainer):\n",
    "\n",
    "    def _get_per_token_logps(self, model, input_ids, attention_mask,\n",
    "                             logits_to_keep):\n",
    "        logits = model(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       logits_to_keep=logits_to_keep + 1).logits\n",
    "        logits = logits[:, :-1, :]\n",
    "        logits = logits[:, -logits_to_keep:]\n",
    "        logits = logits / self.temperature\n",
    "\n",
    "        logits = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(logits)\n",
    "        actions = dist.sample(torch.Size([self.num_generations]))\n",
    "        log_probs = dist.log_prob(actions)  # (N, B, S)\n",
    "\n",
    "        # (B * N, S)\n",
    "        actions = actions.permute(1, 0, 2).view(-1, logits_to_keep)\n",
    "        log_probs = log_probs.permute(1, 0, 2).view(-1, logits_to_keep)\n",
    "\n",
    "        return log_probs, actions\n",
    "\n",
    "    def _generate_and_score_completions(\n",
    "        self, inputs: dict[str, Union[torch.Tensor, Any]]\n",
    "    ) -> dict[str, Union[torch.Tensor, Any]]:\n",
    "        device = self.accelerator.device\n",
    "\n",
    "        prompt_ids = inputs[\"prompt_ids\"][::self.num_generations]\n",
    "        prompt_mask = inputs[\"prompt_mask\"][::self.num_generations]\n",
    "        completion_ids = inputs[\"completion_ids\"][::self.num_generations]\n",
    "        completion_mask = inputs[\"completion_mask\"][::self.num_generations]\n",
    "\n",
    "        if self.max_prompt_length is not None:\n",
    "            prompt_ids = prompt_ids[:, -self.max_prompt_length:]\n",
    "            prompt_mask = prompt_mask[:, -self.max_prompt_length:]\n",
    "\n",
    "        prompt_completion_ids = torch.concat([prompt_ids, completion_ids],\n",
    "                                             dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # When using num_iterations == 1, old_per_token_logps == per_token_logps, so we can skip it's\n",
    "            # computation here, and use per_token_logps.detach() instead.\n",
    "            old_per_token_logps, actions = self._get_per_token_logps(\n",
    "                self.model, prompt_completion_ids, attention_mask,\n",
    "                logits_to_keep)\n",
    "            if self.num_iterations <= 1:\n",
    "                old_per_token_logps = None\n",
    "\n",
    "            if self.beta == 0.0:\n",
    "                ref_per_token_logps = None\n",
    "            elif self.ref_model is not None:\n",
    "                ref_per_token_logps, _ = self._get_per_token_logps(\n",
    "                    self.ref_model, prompt_completion_ids, attention_mask,\n",
    "                    logits_to_keep)\n",
    "            else:\n",
    "                with self.accelerator.unwrap_model(\n",
    "                        self.model).disable_adapter():\n",
    "                    ref_per_token_logps, _ = self._get_per_token_logps(\n",
    "                        self.model, prompt_completion_ids, attention_mask,\n",
    "                        logits_to_keep)\n",
    "\n",
    "        rewards_per_func = torch.zeros(len(actions),\n",
    "                                       len(self.reward_funcs),\n",
    "                                       device=device)  # (B * N, 1)\n",
    "        for i, (reward_func, reward_processing_class) in enumerate(\n",
    "                zip(self.reward_funcs, self.reward_processing_classes)):\n",
    "            if isinstance(\n",
    "                    reward_func, nn.Module\n",
    "            ):  # Module instead of PretrainedModel for compat with compiled models\n",
    "                reward_func_name = f\"reward {reward_func.config._name_or_path.split('/')[-1]}\"\n",
    "            else:\n",
    "                reward_func_name = reward_func.__name__\n",
    "            with profiling_context(self, reward_func_name):\n",
    "                if isinstance(\n",
    "                        reward_func, nn.Module\n",
    "                ):  # Module instead of PretrainedModel for compat with compiled models\n",
    "                    raise NotImplementedError()\n",
    "                else:\n",
    "                    # Repeat all input columns (but \"prompt\" and \"completion\") to match the number of generations\n",
    "                    keys = [\n",
    "                        key for key in inputs[0]\n",
    "                        if not key.startswith(\"prompt\")\n",
    "                    ]\n",
    "                    reward_kwargs = {\n",
    "                        key: [example[key] for example in inputs]\n",
    "                        for key in keys\n",
    "                    }\n",
    "                    output_reward_func = reward_func(\n",
    "                        actions=actions,\n",
    "                        num_generations=self.num_generations,\n",
    "                        **reward_kwargs,\n",
    "                    )\n",
    "                    # Convert None values to NaN\n",
    "                    output_reward_func = [\n",
    "                        reward if reward is not None else torch.nan\n",
    "                        for reward in output_reward_func\n",
    "                    ]\n",
    "\n",
    "                    rewards_per_func[:, i] = torch.tensor(output_reward_func,\n",
    "                                                          dtype=torch.float32,\n",
    "                                                          device=device)\n",
    "\n",
    "        # Gather the reward per function: this part is crucial, because the rewards are normalized per group and the\n",
    "        # completions may be distributed across processes\n",
    "        rewards_per_func = gather(rewards_per_func)  # (B * N, 1)\n",
    "\n",
    "        # Apply weights to each reward function's output and sum\n",
    "        rewards = (rewards_per_func *\n",
    "                   self.reward_weights.to(device).unsqueeze(0)).nansum(dim=1)\n",
    "\n",
    "        # Compute grouped-wise rewards\n",
    "        mean_grouped_rewards = rewards.view(-1,\n",
    "                                            self.num_generations).mean(dim=1)\n",
    "        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)\n",
    "\n",
    "        # Normalize the rewards to compute the advantages\n",
    "        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0)\n",
    "        std_grouped_rewards = std_grouped_rewards.repeat_interleave(\n",
    "            self.num_generations, dim=0)\n",
    "        advantages = rewards - mean_grouped_rewards\n",
    "        if self.args.scale_rewards:\n",
    "            advantages = advantages / (std_grouped_rewards + 1e-4)\n",
    "\n",
    "        # Slice to keep only the local part of the data\n",
    "        process_slice = slice(\n",
    "            self.accelerator.process_index * len(actions),\n",
    "            (self.accelerator.process_index + 1) * len(actions),\n",
    "        )\n",
    "        advantages = advantages[process_slice]\n",
    "\n",
    "        # Log the metrics\n",
    "        mode = \"eval\" if self.control.should_evaluate else \"train\"\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self._total_train_tokens += self.accelerator.gather_for_metrics(\n",
    "                attention_mask.sum()).sum().item()\n",
    "        self._metrics[mode][\"num_tokens\"] = [self._total_train_tokens]\n",
    "\n",
    "        completion_length = self.accelerator.gather_for_metrics(\n",
    "            completion_mask.sum(1)).float().mean().item()\n",
    "        self._metrics[mode][\"completion_length\"].append(completion_length)\n",
    "\n",
    "        # Calculate mean reward per function, but only for samples where the function was applied\n",
    "        for i, reward_func in enumerate(self.reward_funcs):\n",
    "            if isinstance(\n",
    "                    reward_func, nn.Module\n",
    "            ):  # Module instead of PretrainedModel for compat with compiled models\n",
    "                reward_func_name = reward_func.config._name_or_path.split(\n",
    "                    \"/\")[-1]\n",
    "            else:\n",
    "                reward_func_name = reward_func.__name__\n",
    "            # Only calculate mean for samples where this reward function was applied (non-NaN values)\n",
    "            mean_rewards = torch.nanmean(rewards_per_func[:, i]).item()\n",
    "            self._metrics[mode][f\"rewards/{reward_func_name}\"].append(\n",
    "                mean_rewards)\n",
    "        self._metrics[mode][\"reward\"].append(rewards.mean().item())\n",
    "        self._metrics[mode][\"reward_std\"].append(\n",
    "            std_grouped_rewards.mean().item())\n",
    "\n",
    "        return {\n",
    "            \"prompt_ids\": prompt_ids,\n",
    "            \"prompt_mask\": prompt_mask,\n",
    "            \"completion_ids\": completion_ids,\n",
    "            \"completion_mask\": completion_mask,\n",
    "            \"old_per_token_logps\": old_per_token_logps,\n",
    "            \"ref_per_token_logps\": ref_per_token_logps,\n",
    "            \"advantages\": advantages,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0, 1, 1, 1, 0, 1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  0,  0, -1,  1,  0, -1,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3, 3)[1, 2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
